from __future__ import annotations

import csv
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

import numpy as np
import yaml

from pulsedb_subset.pipeline.config import load_config
from pulsedb_subset.pipeline.eda_labels import decode_subject_id, read_vector
from pulsedb_subset.pipeline.io_mat import load_json


def _log(msg: str) -> None:
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")


def main() -> int:
    """Generate global subject-disjoint train/val/test split.

    Outputs (artifacts/):
    - subject_splits.yaml: subject_id lists per split + seed + fractions
    - subject_overlap.csv: pairwise overlaps between provided subsets
    - split_stats_by_subset.csv: n_subjects/n_segments per subset per split

    Important:
    - Split is GLOBAL by subject_id across all subsets.
    - Downstream caching/training must filter by these subject sets to avoid leakage.
    """

    _log("02_make_subject_splits: start")
    cfg = load_config()
    cfg.artifacts_dir.mkdir(parents=True, exist_ok=True)

    schema_path = cfg.artifacts_dir / "schema_map.json"
    if not schema_path.exists():
        raise FileNotFoundError(f"Missing {schema_path}. Run scripts/01_inventory_schema.py first.")
    schemas = load_json(schema_path)

    seed = int(getattr(cfg, "splits_seed", cfg.eda_random_seed))
    train_frac = float(getattr(cfg, "splits_train_frac", 0.8))
    val_frac = float(getattr(cfg, "splits_val_frac", 0.1))

    if not (0.0 < train_frac < 1.0) or not (0.0 <= val_frac < 1.0) or (train_frac + val_frac >= 1.0):
        raise ValueError(f"Invalid split fractions train={train_frac} val={val_frac}")

    subject_ids_by_subset: dict[str, np.ndarray] = {}
    for subset_name, filename in cfg.subset_files.items():
        mat_path = (cfg.subset_mat_dir / filename).resolve()
        ds_map = (schemas.get(subset_name, {}) or {}).get("dataset_map", {})
        sid_path = ds_map.get("subject_id")

        # Use SBP/DBP length as the ground-truth segment count for decoding.
        sbp = read_vector(mat_path, ds_map.get("sbp"))
        dbp = read_vector(mat_path, ds_map.get("dbp"))
        expected_n = int(max(sbp.size, dbp.size))
        if expected_n <= 0:
            expected_n = None

        sid_raw = read_vector(mat_path, sid_path)
        sid = decode_subject_id(sid_raw, expected_n=expected_n)
        if expected_n is not None and sid.size != expected_n:
            raise RuntimeError(
                f"subset={subset_name}: decoded subject_id length mismatch: got={sid.size} expected={expected_n}"
            )
        subject_ids_by_subset[subset_name] = sid
        _log(f"subset={subset_name}: subject_id n={sid.size} unique={int(np.unique(sid).size) if sid.size else 0}")

    all_subjects = np.unique(np.concatenate([v for v in subject_ids_by_subset.values() if v.size], axis=0))
    all_subjects = all_subjects[np.isfinite(all_subjects)]
    all_subjects = all_subjects.astype(np.int64, copy=False)
    all_subjects = np.unique(all_subjects)

    if all_subjects.size < 3:
        raise RuntimeError(f"Too few unique subjects to split: {int(all_subjects.size)}")

    rng = np.random.default_rng(seed)
    perm = rng.permutation(all_subjects.size)
    all_shuf = all_subjects[perm]

    n_total = int(all_shuf.size)
    n_train = max(1, int(round(train_frac * n_total)))
    n_val = max(0, int(round(val_frac * n_total)))

    # keep at least 1 test subject
    if n_train + n_val >= n_total:
        n_val = max(0, n_total - n_train - 1)
    n_test = n_total - n_train - n_val
    if n_test <= 0:
        raise RuntimeError("Split resulted in empty test set")

    train_subjects = np.sort(all_shuf[:n_train])
    val_subjects = np.sort(all_shuf[n_train : n_train + n_val])
    test_subjects = np.sort(all_shuf[n_train + n_val :])

    out_yaml = cfg.artifacts_dir / "subject_splits.yaml"
    payload = {
        "date_note": "Generated by scripts/02_make_subject_splits.py",
        "seed": seed,
        "train_frac": float(train_frac),
        "val_frac": float(val_frac),
        "test_frac": float(1.0 - train_frac - val_frac),
        "n_subjects_total": int(n_total),
        "n_subjects_train": int(train_subjects.size),
        "n_subjects_val": int(val_subjects.size),
        "n_subjects_test": int(test_subjects.size),
        "train_subjects": [int(x) for x in train_subjects.tolist()],
        "val_subjects": [int(x) for x in val_subjects.tolist()],
        "test_subjects": [int(x) for x in test_subjects.tolist()],
    }
    out_yaml.write_text(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True), encoding="utf-8")
    _log(f"wrote: {out_yaml}")

    # Pairwise overlaps (documents the problem + sanity check)
    out_overlap = cfg.artifacts_dir / "subject_overlap.csv"
    subset_names = list(cfg.subset_files.keys())
    sets = {k: set(np.unique(v).tolist()) for k, v in subject_ids_by_subset.items()}

    with out_overlap.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(
            f,
            fieldnames=["subset_a", "subset_b", "n_subjects_a", "n_subjects_b", "n_overlap", "overlap_example"],
        )
        w.writeheader()
        for i, a in enumerate(subset_names):
            for b in subset_names[i + 1 :]:
                sa = sets.get(a, set())
                sb = sets.get(b, set())
                inter = sorted(list(sa.intersection(sb)))
                w.writerow(
                    {
                        "subset_a": a,
                        "subset_b": b,
                        "n_subjects_a": len(sa),
                        "n_subjects_b": len(sb),
                        "n_overlap": len(inter),
                        "overlap_example": ";".join(str(x) for x in inter[:10]),
                    }
                )
    _log(f"wrote: {out_overlap}")

    # Split stats per subset
    out_stats = cfg.artifacts_dir / "split_stats_by_subset.csv"

    train_set = set(int(x) for x in train_subjects.tolist())
    val_set = set(int(x) for x in val_subjects.tolist())
    test_set = set(int(x) for x in test_subjects.tolist())

    def _split_name(sid: int) -> str:
        if sid in train_set:
            return "train"
        if sid in val_set:
            return "val"
        if sid in test_set:
            return "test"
        return "unknown"

    rows = []
    for subset_name, sid in subject_ids_by_subset.items():
        if sid.size == 0:
            continue
        split_arr = np.array([_split_name(int(x)) for x in sid.tolist()], dtype=object)
        for split in ["train", "val", "test"]:
            mask = split_arr == split
            sids = sid[mask]
            rows.append(
                {
                    "subset_name": subset_name,
                    "split": split,
                    "n_segments": int(mask.sum()),
                    "n_subjects": int(np.unique(sids).size) if sids.size else 0,
                }
            )

    with out_stats.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["subset_name", "split", "n_segments", "n_subjects"])
        w.writeheader()
        w.writerows(rows)
    _log(f"wrote: {out_stats}")

    _log("02_make_subject_splits: done")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
